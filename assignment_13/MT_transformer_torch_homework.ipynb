{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Транформеры для решения seq2seq задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq - наверное самая общая формальная постановка задачи в NLP. Нужно из произвольной последовательности получить какую-то другую последовательность. И в отличие от разметки последовательности (sequence labelling) не требуется, чтобы обе последовательности совпадали по длине. Даже стандартную задачу классификации можно решать как seq2seq - можно рассматривать метку класса как последовательность длинны 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А трансформеры - sota архитектура для seq2seq задач. Мы не будем подробно разбирать устройство транформеров, если вам интересно вы можете поразбираться вот с этими материалами:\n",
    "\n",
    "Оригинальная статья (сложновато) - https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  \n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://www.youtube.com/watch?v=iDulhoQ2pro\n",
    "\n",
    "https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
    "\n",
    "Самый известный туториал (на торче) - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "\n",
    "\n",
    "Трансформеры будут подробно разбираться на курсе глубокого обучения (по выбору) на втором курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока просто попробуем обучать модель на задаче машинного перевода. Для таких задач лучше всего использовать предобученные модели, но если у вас будет какая-то специфичная seq2seq задача, то имеет смысл попробовать обучить трансформер с нуля и в этой тертрадке вам нужно будет поменять только часть с загрузкой данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"./torch_weights/tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"./torch_weights/tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. Вы поймете почему именно cls и sep, а не start и end, если подробнее поразбираетесь с устройством трансформеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "max_len_en, max_len_ru = 30, 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Код трансформера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "Там есть комментарии по каждому этапу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 150):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size, \n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "#         print('pos inp')\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "#         print('pos dec')\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "#         print('pos out')\n",
    "        x = self.generator(outs)\n",
    "#         print('gen')\n",
    "        return x\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то как мы подаем данные в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "RU_VOCAB_SIZE = tokenizer_ru.get_vocab_size()\n",
    "\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, EN_VOCAB_SIZE, RU_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = torch.load(\"./torch_weights/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Homework starts here\n",
    "\n",
    "Disclaime: Этой домашкой я бы хотел закрыть пропуски в домашках 6, 8, 9. Остальные (скорее всего 10/11/12) я постараюсь досдать\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(texts: List[str], max_len: int) -> Tuple[Tensor, Tensor]:\n",
    "    encodings = tokenizer_en.encode_batch(texts)\n",
    "    encodings = [\n",
    "        [tokenizer_en.token_to_id('[CLS]')] + encoding.ids[:max_len] + [tokenizer_en.token_to_id('[SEP]')]\n",
    "        for encoding in encodings\n",
    "    ]\n",
    "    outputs = [[tokenizer_ru.token_to_id('[CLS]')]]*len(texts)\n",
    "    \n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.LongTensor(input_ids) for input_ids in encodings],\n",
    "        batch_first=False,\n",
    "        padding_value=PAD_IDX\n",
    "    ).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.LongTensor(output_ids) for output_ids in outputs],\n",
    "        batch_first=False,\n",
    "        padding_value=PAD_IDX\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    return input_ids_pad, output_ids_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_IDX = tokenizer_ru.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_decode(output_ids_pad: Tensor) -> List[str]:\n",
    "    batch_size = output_ids_pad.shape[1]\n",
    "    decode = []\n",
    "    for sequence_idx in range(batch_size):\n",
    "        sequence = output_ids_pad[:, sequence_idx].cpu().numpy() # to ensure it is on cpu\n",
    "        filtered_sequence = []\n",
    "        for token_id in sequence:\n",
    "            if token_id not in {PAD_IDX, SEP_IDX}:\n",
    "                filtered_sequence.append(token_id)\n",
    "            else: # found sep or pad, stopping\n",
    "                break\n",
    "                \n",
    "        decode.append(tokenizer_ru.decode(filtered_sequence))\n",
    "    \n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts: List[str], max_input_len: int = 30, max_output_len: int = 35): # now working with batches!\n",
    "\n",
    "    input_ids_pad, output_ids_pad = batch_encode(texts, max_input_len)\n",
    "\n",
    "    (texts_en_mask, texts_ru_mask, \n",
    "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "    \n",
    "    pred = torch.softmax(logits, -1).argmax(-1) # it needs softmaxing\n",
    "    for i in range(max_output_len):\n",
    "        output_ids_pad = torch.cat(\n",
    "            (output_ids_pad, pred)\n",
    "        )\n",
    "        (texts_en_mask, texts_ru_mask, \n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "        \n",
    "        # argmax over last token + unsqueeze to create seq_length dimension\n",
    "        pred = torch.softmax(logits, -1).argmax(-1)[-1].unsqueeze(0)\n",
    "\n",
    "    return batch_decode(output_ids_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пример', 'Еще один жесто кий и супер зло го мега пример']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate([\"Example\", \"Also another cruel and super-evil megaexample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_news_text = \"\"\"\n",
    "More than half of U.S. states have lowered some barriers to voting\n",
    "since the 2020 election, making permanent practices that helped\n",
    "produce record voter turnout during the coronavirus pandemic — a\n",
    "striking countertrend to the passage this year of restrictions in\n",
    "key Republican-controlled states.\n",
    "New laws in states from Vermont to California expand access to the\n",
    "voting process on a number of fronts, such as offering more options\n",
    "for early and mail voting, protecting mail ballots from being improperly\n",
    "rejected and making registering to vote easier.\n",
    "Some states restored voting rights to people with past felony convictions\n",
    "or expanded options for voters with disabilities, two long-standing priorities\n",
    "among voting advocates. And in Virginia, a new law requires localities\n",
    "to receive preapproval or feedback on voting changes as a shield against\n",
    "racial discrimination, a first for states after the Supreme Court struck\n",
    "down a key part of the federal Voting Rights Act in 2013.\n",
    "Kentucky Secretary of State Michael Adams, a Republican who fought for his\n",
    "state’s policy changes, said the GOP needs to “stop being scared of voters.”\n",
    "“Let them vote, and go out and make the case,” he said in an interview,\n",
    "adding: “I want Republicans to succeed. I think it’s an unforced error to\n",
    "shoot themselves in the foot in these states by shrinking access. You don’t need to do that.”\n",
    "Seventy-one new laws easing voting rules are poised to benefit 63 million \n",
    "eligible voters in 28 states, or about one-quarter of the U.S. voting population,\n",
    "according to the Voting Rights Lab report, which tracked policy changes as of June 13.\n",
    "Thirty-one new laws in 18 states create more barriers to the ballot box,\n",
    "affecting 36 million eligible voters, or 15 percent of the national voting\n",
    "population, the report stated.\n",
    "Legislative debates over restrictions are underway in key states such as Texas\n",
    "and Pennsylvania, leaving open the possibility that new limitations affecting\n",
    "millions more voters still will be enacted this year.\n",
    "\"\"\"\n",
    "## part of text from here: https://www.washingtonpost.com/politics/voting-rights-expansion-states/2021/06/22/1699a6b0-cf87-11eb-8014-2f3926ca24d9_story.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence-splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_splitter import SentenceSplitter\n",
    "splitter = SentenceSplitter('en')\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "        \n",
    "        \n",
    "# target function to translate big texts\n",
    "# works by splitting text into sentences and batched translation of those sentences\n",
    "def translate_big_text(text, batch_size: int = 8):\n",
    "    sentences = splitter.split(text)\n",
    "    translated_sentences = []\n",
    "    for sentences_batch in batch(sentences, n=batch_size):\n",
    "        translated_sentences += translate(sentences_batch, 100, 100)\n",
    "        \n",
    "    return \".\\n\".join(translated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Более половины американских государств снизи ли некоторые барье ры для голосования ..\n",
      "С 2020 года выборы , принятие постоянных практики , которые помогли.\n",
      "Вы можете получить запись избирателей во время панде мии в панде мии : a.\n",
      "С мет ровая тенденция к принятию в этом году ограничений в отношении ограничения.\n",
      "Клю чи республикан ские под контролем ..\n",
      "Новые законы в шта тах Вер монт и Калифорния расши ряет доступ к.\n",
      "Процесс голосования по ряду направлений , таких , как предложение больше вариантов.\n",
      "для раннего и почта голосования , защиту электронной почте в качестве официального утверждения , в отношении того , чтобы они были должным образом.\n",
      "откло нил и регистри ру ясь на голосование ..\n",
      "Некоторые государства восстано вили права на голосование в отношении лиц , имеющих прошлое обви нение в убийстве.\n",
      "или расши ряет варианты избирателей с ограниченными возможностями , два долго стоя тельства.\n",
      "Среди участников голосования ..\n",
      "И в Вирджи нии новый закон требует местных особенностей.\n",
      "до получения предварительного утверждения или от кли ники о внесении изменений в качестве щи та против.\n",
      "Ра совый состав населения : первый раз , когда Верховный суд был нанесен а в соответствии с положениями Верховного суда ..\n",
      "В 2013 году в 2013 году в качестве одного из основных элементов Закона о правах голосов ..\n",
      "Кен ту кки , Майкл Адам с , рес публика нец , сража вшийся за его пределами ..\n",
      "Политика изменения государства , сказала , что ГО П должны « остановить быть напуга ны избирателей »..\n",
      "« Пусть они голосу ют и выходят за дело », - сказал он в интервью ,.\n",
      "доба влять : « Я хочу , чтобы Республика ans преуспе ли ..\n",
      "Я думаю , это не прину жден ная ошибка.\n",
      "В этих государствах стреля ют в них , в которых они будут сокра щаться ..\n",
      "Вам не нужно это делать »..\n",
      "Семь десят одно новое законодательство по ослаб лению голосования составляют 63 миллиона х.\n",
      "человек , который должен быть принят в 28 шта тах , или около одного квартала американских жителей США ..\n",
      "Согласно докладу Ла бора тория по вопросам прав человека , который отсле дил изменения политики в области политики 13 июня ..\n",
      "Тридцать один новый закон в 18 государствах создают более препятствия для голосования ,.\n",
      "затраги вающие 36 миллионов человек , имеющие право голоса , или 15 процентов национального голосования.\n",
      "Население , по данным , приведен ному в докладе ..\n",
      "В ключевых государствах , таких как Техас.\n",
      "и Пенсильвания , поки ну я возможность открыть новые ограничения , затраги вающие.\n",
      "В этом году в этом году все еще будут приняты миллионы избирателей .\n"
     ]
    }
   ],
   "source": [
    "print(translate_big_text(big_news_text, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
