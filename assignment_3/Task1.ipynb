{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize, tokenize\n",
    "from tqdm.auto import tqdm\n",
    "from dawg import BytesDAWG, IntDAWG\n",
    "from nltk import ngrams\n",
    "\n",
    "import textdistance as td\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "punct = set(string.punctuation + \"«»—…“”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/correct_sents.txt\", 'r', encoding='utf-8') as f:\n",
    "    correct_sents = f.readlines()\n",
    "    \n",
    "with open(\"data/sents_with_mistakes.txt\", 'r', encoding='utf-8') as f:\n",
    "    sents_with_mistakes = f.readlines()\n",
    "    \n",
    "with open(\"data/wiki_data.txt\", 'r', encoding='utf-8') as f:\n",
    "    wiki_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(916, 916, 20002)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_sents), len(sents_with_mistakes), len(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sents = list(map(lambda s: re.sub(r'[^\\w\\s]','',s).replace(\"  \", \" \"),  correct_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_with_mistakes = list(map(lambda s: re.sub(r'[^\\w\\s]','',s).replace(\"  \", \" \"), sents_with_mistakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = list(map(lambda s: s.replace(\"#\", ' '), wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed603fa3fb84194877cb16a94494e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20002.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_sentences = []\n",
    "for text in tqdm(wiki_data):\n",
    "    sentences = list(map(\n",
    "        lambda s: re.sub(\n",
    "            r'[^\\w\\s]',\n",
    "            ' ',\n",
    "            s.replace(\")\", \" \").replace(\"(\", \" \")\n",
    "        ).replace(\"  \", \" \").replace(\"  \", \" \").replace(\"  \", \" \").strip(), # squash multiple space in the middle\n",
    "        map(\n",
    "            lambda x: x.text,\n",
    "            sentenize(text)\n",
    "        )\n",
    "    ))\n",
    "    wiki_sentences += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb6e6549f6b4230ac02276ac6c8644e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=217867.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigrams = []\n",
    "bigrams  = []\n",
    "trigrams = []\n",
    "for sent in tqdm(wiki_sentences):\n",
    "    words = [\"<start>\"] + list(map(lambda x: x.text.lower(), tokenize(sent))) + [\"<end>\"]\n",
    "    unigrams += words\n",
    "    bigrams += ngrams(words, n=2)\n",
    "    trigrams += ngrams(words, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_frequencies = Counter(unigrams)\n",
    "bigram_frequencies = Counter(bigrams)\n",
    "trigram_frequencies = Counter(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(unigram_frequencies.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_deletions(word: str, n_deletions: int = 2) -> List[str]:\n",
    "    first_order = []\n",
    "    for i in range(len(word)):\n",
    "        variant = word[:i] + word[i+1:]\n",
    "        first_order.append(variant)\n",
    "        \n",
    "    if n_deletions == 1:\n",
    "        return list(set(first_order))\n",
    "    else:\n",
    "        second_order = []\n",
    "        for variant in first_order:\n",
    "            second_order += gen_deletions(variant, n_deletions-1)\n",
    "            \n",
    "        return list(set(first_order + second_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401d6ed443794aa7b95ccd9fb74ef3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=365947.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "symspell_index = []\n",
    "for i, word in enumerate(tqdm(vocab)):\n",
    "    _word = re.sub(r'[0-9]+', '', word)\n",
    "    if _word == word and len(word) > 2:\n",
    "        variants = gen_deletions(_word, 2)\n",
    "        for v in variants:\n",
    "            if len(v) > 2:\n",
    "                symspell_index.append((v, i.to_bytes(4, 'little')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Можно конечно хранить индекс в питоновском dict, но при сериализации в json он занимал на диске 3гб места, и вероятно очень много в оперативке\n",
    "\n",
    "Вместо питоновского dict, я использую альтернативный вариант с dawg-контейнерами, которые позволяют хранить пары вида строковый_ключ -> список байтовый массивов, строковый_ключ -> int-число c очень очень маленьким расходом памяти и таким-же временем доступа, как и у диктов\n",
    "\n",
    "А ещё эта штука используется в fast-варианте pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symspell_index = BytesDAWG(symspell_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_frequencies = unigram_frequencies.most_common()\n",
    "bigram_frequencies = list(map(lambda x: (\" \".join(x[0]), x[1]), bigram_frequencies.most_common()))\n",
    "trigram_frequencies = list(map(lambda x: (\" \".join(x[0]), x[1]), trigram_frequencies.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_index = IntDAWG(unigram_frequencies)\n",
    "bigrams_index = IntDAWG(bigram_frequencies)\n",
    "trigrams_index = IntDAWG(trigram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_index.save(\"unigrams.index\")\n",
    "bigrams_index.save(\"bigrams.index\")\n",
    "trigrams_index.save(\"trigrams.index\")\n",
    "symspell_index.save(\"symspell.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel BZIP2 v1.1.9     - by: Jeff Gilchrist [http://compression.ca]\n",
      "[Apr. 13, 2014]               (uses libbzip2 by Julian Seward)\n",
      "Major contributions: Yavor Nikolov <nikolov.javor+pbzip2@gmail.com>\n",
      "\n",
      "         # CPUs: 16\n",
      " BWT Block Size: 900 KB\n",
      "File Block Size: 900 KB\n",
      " Maximum Memory: 100 MB\n",
      "-------------------------------------------\n",
      "         File #: 1 of 4\n",
      "     Input Name: symspell.index\n",
      "    Output Name: symspell.index.bz2\n",
      "\n",
      "     Input Size: 101412872 bytes\n",
      "Compressing data...\n",
      "    Output Size: 68295847 bytes\n",
      "-------------------------------------------\n",
      "         File #: 2 of 4\n",
      "     Input Name: unigrams.index\n",
      "    Output Name: unigrams.index.bz2\n",
      "\n",
      "     Input Size: 3149828 bytes\n",
      "Compressing data...\n",
      "    Output Size: 1919098 bytes\n",
      "-------------------------------------------\n",
      "         File #: 3 of 4\n",
      "     Input Name: bigrams.index\n",
      "    Output Name: bigrams.index.bz2\n",
      "\n",
      "     Input Size: 35648516 bytes\n",
      "Compressing data...\n",
      "    Output Size: 20356333 bytes\n",
      "-------------------------------------------\n",
      "         File #: 4 of 4\n",
      "     Input Name: trigrams.index\n",
      "    Output Name: trigrams.index.bz2\n",
      "\n",
      "     Input Size: 136350724 bytes\n",
      "Compressing data...\n",
      "    Output Size: 62248882 bytes\n",
      "-------------------------------------------\n",
      "\n",
      "     Wall Clock: 2.372173 seconds\n"
     ]
    }
   ],
   "source": [
    "!pbzip2 -kzvf -9 symspell.index unigrams.index bigrams.index trigrams.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  20M Dec  5 12:54 bigrams.index.bz2\n",
      "-rw-r--r-- 1 root root  66M Dec  5 12:54 symspell.index.bz2\n",
      "-rw-r--r-- 1 root root  60M Dec  5 12:54 trigrams.index.bz2\n",
      "-rw-r--r-- 1 root root 1.9M Dec  5 12:54 unigrams.index.bz2\n"
     ]
    }
   ],
   "source": [
    "!ls -lh *.index.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_symspell(word, vocab: List[str], symspell_index: BytesDAWG):\n",
    "    variants = gen_deletions(word, n_deletions=2)\n",
    "    candidates = []\n",
    "    for v in variants:\n",
    "        _binary = symspell_index.get(v, None)\n",
    "        if _binary:\n",
    "            candidates += list(\n",
    "                map(\n",
    "                    lambda x: vocab[int.from_bytes(x, 'little')], # декодируем \n",
    "                    _binary # в таком порядке: bytes -> int(индекс слова в массиве) -> str(само слово)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    candidates = list(set(candidates))\n",
    "    candidates = list(map(lambda x: (x, td.damerau_levenshtein(x, word)), candidates))\n",
    "    candidates = sorted(candidates, key=lambda x: x[1])\n",
    "    return list(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_by_ngram(\n",
    "    word, context, unigrams, bigrams, trigrams,\n",
    "    total_words_count: int, weights = (0.1, 0.1, 0.8), do_trigram_only=False\n",
    ") -> float:\n",
    "    score = 0.0\n",
    "    if word not in unigrams:\n",
    "        return 0.0\n",
    "        \n",
    "    unigram_weight, bigram_weight, trigram_weigth = weights\n",
    "    \n",
    "    if not do_trigram_only:\n",
    "        unigram_score = unigrams.get(word, 0.0) / total_words_count\n",
    "        score += unigram_score * unigram_weight\n",
    "        if len(context) >= 1:\n",
    "            bigram_score = bigrams.get(f\"{context[-1]} {word}\", 0.0) / unigrams.get(context[-1], 1.0)\n",
    "            score += bigram_score * bigram_weight\n",
    "        else:\n",
    "            score += bigram_weight\n",
    "        \n",
    "    if len(context) >= 2:\n",
    "        trigram_score = trigrams.get(\n",
    "            f\"{context[-2]} {context[-1]} {word}\", 0.0\n",
    "        ) / bigrams.get(f\"{context[-2]} {context[-1]}\", 1.0)\n",
    "        score += trigram_score * trigram_weigth\n",
    "    else:\n",
    "        score += trigram_weigth\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_count = sum(unigrams_index.get(word) for word in vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcorrect(sentence, vocab, vocabset, symspell_index, unigrams_index, bigrams_index, trigrams_index, total_words_count, do_ngram=True):\n",
    "    words = [\"<start>\"] + [x.text for x in tokenize(sentence)] + [\"<end>\"]\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in vocabset:\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        candidates = get_matches_symspell(word, vocab, symspell_index)\n",
    "        if len(candidates) == 0:\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        max_dist = max(x[1] for x in candidates)\n",
    "        scored_candidates = []\n",
    "        for candidate, dist in candidates:\n",
    "            context = words[max(i-2, 0):i]\n",
    "            if do_ngram: # если будем ранжировать варианты по n-gramm вероятностям\n",
    "                lm_score = score_by_ngram(\n",
    "                    candidate, context,\n",
    "                    unigrams_index,\n",
    "                    bigrams_index,\n",
    "                    trigrams_index,\n",
    "                    total_words_count\n",
    "                )\n",
    "            else: # иначе оставим у всех одинаковый скор\n",
    "                lm_score = 1.0\n",
    "            score = ((max_dist + 1) - dist)*lm_score # нужно чтобы у слова с меньшей дистанцией был наивысший скор\n",
    "            scored_candidates.append((candidate, score))\n",
    "            \n",
    "        best_candidate, best_score = sorted(scored_candidates, key=lambda x: x[1])[-1]\n",
    "        corrected_words.append(best_candidate)\n",
    "            \n",
    "    return \" \".join(corrected_words[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_with_mistakes = list(x.lower().replace(\"\\n\", \"\") for x in sents_with_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1364c5ef4b2d4a2abd77f3408d55f935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=916.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# исправляем с ngram-ранжированием\n",
    "vocabset = set(vocab) # set(..) проще сделать один раз чем ждать пока его на каждой итерации будет делать функция\n",
    "corrected_ngram = []\n",
    "for sent in tqdm(sents_with_mistakes):\n",
    "    correction = spellcorrect(\n",
    "        sent.lower(), vocab, vocabset,\n",
    "        symspell_index,\n",
    "        unigrams_index,\n",
    "        bigrams_index, \n",
    "        trigrams_index,\n",
    "        total_words_count\n",
    "    )\n",
    "    corrected_ngram.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525bb52deb4848a38f325df02daa1062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=916.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabset = set(vocab)\n",
    "corrected_symspell = []\n",
    "# исправляем только по индексу + дистанции\n",
    "for sent in tqdm(sents_with_mistakes):\n",
    "    correction = spellcorrect(\n",
    "        sent.lower(), vocab, vocabset,\n",
    "        symspell_index,\n",
    "        unigrams_index,\n",
    "        bigrams_index, \n",
    "        trigrams_index,\n",
    "        total_words_count,\n",
    "        do_ngram=False\n",
    "    )\n",
    "    corrected_symspell.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2, sent_3):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    tokens_3 = sent_3.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    tokens_3 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_3 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2, tokens_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(original, corrected, reference):\n",
    "    correct, total = 0.0, 0.0\n",
    "    mistaken_fixed, mistaken_total = 0.0, 0.0\n",
    "    correct_broken, correct_total = 0.0, 0.0\n",
    "    for s_orig, s_corrected, s_reference in tqdm(zip(original, corrected, reference)):\n",
    "        for word_orig, word_corrected, word_reference in align_words(s_orig, s_corrected, s_reference):\n",
    "            total += 1.0\n",
    "            if word_corrected == word_reference:\n",
    "                correct += 1.0\n",
    "                \n",
    "            if word_orig == word_reference:\n",
    "                correct_total += 1.0\n",
    "                if word_corrected != word_reference:\n",
    "                    correct_broken += 1.0\n",
    "            else:\n",
    "                mistaken_total += 1.0\n",
    "                if word_corrected == word_reference:\n",
    "                    mistaken_fixed += 1.0\n",
    "                    \n",
    "    return {\n",
    "        'correct_ratio': correct/total,\n",
    "        'mistakes_correct_ratio': mistaken_fixed/mistaken_total,\n",
    "        'broken_ratio': correct_broken/correct_total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bcc4f4712949c3be6bc3176adf8838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'correct_ratio': 0.8447552447552448, 'mistakes_correct_ratio': 0.4191235059760956, 'broken_ratio': 0.09423186750428326}\n"
     ]
    }
   ],
   "source": [
    "## for symspell-only model\n",
    "print(metrics(sents_with_mistakes, corrected_symspell, correct_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007150a644b04b6b8a8ae8fd59c1f78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'correct_ratio': 0.8419580419580419, 'mistakes_correct_ratio': 0.39681274900398406, 'broken_ratio': 0.09423186750428326}\n"
     ]
    }
   ],
   "source": [
    "## for symspell + ngram model\n",
    "print(metrics(sents_with_mistakes, corrected_ngram, correct_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение:\n",
    "Почему-то модель без ngram работает немножечко лучше, чем модель с ngram.\n",
    "Есть две возможные причины:\n",
    "1. я где-то накосячил в расчете вероятностей(но если заменить log-exp - то получаются ещё хуже метрики)\n",
    "2. так сложилось - так загрузился датасет, посчитался индекс и иные причины, не зависящие напрямую от меня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
