{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 1 - триграмная языковая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "далее идет повторение части кода из тетрадки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_memory_usage():\n",
    "    ''' Memory usage in gB '''\n",
    "\n",
    "    with open('/proc/self/status') as f:\n",
    "        memusage = f.read().split('VmRSS:')[1].split('\\n')[0][:-3]\n",
    "\n",
    "    return f\"{int(memusage.strip()) / 1024.0 / 1024.0:.4f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0470 GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "dvach = open('data/2ch_corpus.txt').read()\n",
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "\n",
    "news = open('data/lenta.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По длине оно сопоставимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 1 - 11638405\n",
      "Длина 2 - 11536552\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина 1 -\", len(dvach))\n",
    "print(\"Длина 2 -\", len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию для нормализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним тексты по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса токсичных постов в токенах - 1858941\n",
      "Длина корпуса новостных текстов в токенах -  1505789\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И по уникальным токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных токенов в корпусе токсичных постов - 189515\n",
      "Уникальный токенов в корпусе новостных текстов -  116302\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных токенов в корпусе токсичных постов -\", len(set(norm_dvach)))\n",
    "print(\"Уникальный токенов в корпусе новостных текстов - \", len(set(norm_news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e925399deffa4c9e8ecb9b351051f499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=170507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a408aa15f14c80aa40ba91b0e149d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_dvach = [['<start>'] + normalize(text) + ['<end>'] for text in tqdm(sent_tokenize(dvach))]\n",
    "sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in tqdm(sent_tokenize(news))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А вот тут помимо bigrams_* создаем и trigrams_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1dd9ee51b74b398bdcbda753edccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=170507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21fd4763b3546b5a3b657b01d758313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in tqdm(sentences_dvach):\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in tqdm(sentences_news):\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> об этом', 1579),\n",
       " ('<start> по словам', 1549),\n",
       " ('сообщает риа новости', 1324),\n",
       " ('со ссылкой на', 1243),\n",
       " ('риа новости <end>', 1228),\n",
       " ('<start> кроме того', 1070),\n",
       " ('<start> как сообщает', 1064),\n",
       " ('<start> напомним что', 1006),\n",
       " ('по его словам', 899),\n",
       " ('<start> по его', 868)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут пробуем считать вероятность по триграммам, нормируя на количество вхождений биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала''Ныть надо меньше и работать больше.'\n",
    "phrase = 'как вы смотрите эту залупу, серьезно. в чем прикол ваще это смотреть?'\n",
    "prob = Counter()\n",
    "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>'], n=3):\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    \n",
    "    if ' '.join([word1, word2]) in bigrams_dvach and ngram in trigrams_dvach:\n",
    "        prob['dvach'] += np.log(trigrams_dvach[ngram]/bigrams_dvach[' '.join([word1, word2])])\n",
    "    else:\n",
    "        prob['dvach'] += np.log(0.001)\n",
    "    \n",
    "    if ' '.join([word1, word2]) in bigrams_news and ngram in trigrams_news:\n",
    "        prob['news'] += np.log(trigrams_news[ngram]/bigrams_news[' '.join([word1, word2])])\n",
    "    else:\n",
    "        prob['news'] += np.log(0.001)\n",
    "\n",
    "prob['news'] = np.exp(prob['news'])\n",
    "prob['dvach'] = np.exp(prob['dvach'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает получше, чем вариант с биграммами, вероятности отличаются на 15 порядков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dvach', 1.6929191136207173e-14), ('news', 1.0000000000000028e-36)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собираем матрицу вероятностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "умещаем вероятности в одну матрицу - сначала идут униграммы, дальше биграммы(хотя по идее униграммы тут уже и не нужны)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apparently пришлось использовать fp16 для матрицы двача, т.к иначе не хватало даже имеющегося терабайта RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5966 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e021f15cfc914cf686051e6d7e00631d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1032320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4eafacbc0a4921be067c5539662a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1528620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_dvach = np.zeros(\n",
    "    (len(unigrams_dvach)+len(bigrams_dvach), len(unigrams_dvach)), dtype=np.float16\n",
    ")\n",
    "print(get_current_memory_usage())\n",
    "id2word_dvach = list(unigrams_dvach)+list(bigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "# заполняем вероятности биграммной части\n",
    "for ngram in tqdm(bigrams_dvach):\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_dvach[word2id_dvach[word1]][word2id_dvach[word2]] =  (bigrams_dvach[ngram]/\n",
    "                                                                     unigrams_dvach[word1])\n",
    "# заполняем вероятности триграммной части\n",
    "for ngram in tqdm(trigrams_dvach):\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = \" \".join([word1, word2])\n",
    "    matrix_dvach[word2id_dvach[bigram]][word2id_dvach[word3]] = trigrams_dvach[ngram]/bigrams_dvach[bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          1.0Ti        76Gi       388Gi       2.0Mi       542Gi       925Gi\n",
      "Swap:         8.0Gi       9.0Mi       8.0Gi\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.2325 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56670505128940e6b67054ebceb8badf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=769662.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56c6e35903a44e8a19293ce06782c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1200381.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_news = np.zeros(\n",
    "    (len(unigrams_news)+len(bigrams_news), len(unigrams_news)), dtype=np.float16\n",
    ")\n",
    "print(get_current_memory_usage())\n",
    "\n",
    "id2word_news = list(unigrams_news)+list(bigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "# заполняем вероятности биграммной части\n",
    "for ngram in tqdm(bigrams_news):\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_news[word2id_news[word1]][word2id_news[word2]] =  (bigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])\n",
    "# заполняем вероятности триграммной части\n",
    "for ngram in tqdm(trigrams_news):\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = \" \".join([word1, word2])\n",
    "    matrix_news[word2id_news[bigram]][word2id_news[word3]] = trigrams_news[ngram]/bigrams_news[bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          1.0Ti        82Gi       382Gi       2.0Mi       542Gi       919Gi\n",
      "Swap:         8.0Gi       9.0Mi       8.0Gi\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация по триграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    next_word = id2word[np.random.choice(matrix.shape[1], p=matrix[current_idx])] \n",
    "    ## генерируем второе слово\n",
    "    ## чтобы дальше генерировать только по биграммам\n",
    "    text += [next_word]\n",
    "    current_idx = word2id[\" \".join([start, next_word])] \n",
    "    ## current_idx = id сгенерированной биграммы вида '<start> word'\n",
    "    \n",
    "    for i in range(n):\n",
    "        chosen_idx = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        prev_word = text[-1]\n",
    "        text.append(id2word[chosen_idx])\n",
    "        chosen = word2id[\" \".join([prev_word, id2word[chosen_idx]])]  \n",
    "        ## следующий id - id биграммы\n",
    "        \n",
    "        if id2word[chosen_idx] == '<end>': \n",
    "            ## если встретили <end> - повторяем процедуру из начала генерации\n",
    "            chosen = word2id['<start>']\n",
    "            next_word_idx = np.random.choice(matrix.shape[1], p=matrix[chosen])\n",
    "            text.append(id2word[next_word_idx])\n",
    "            chosen = word2id[\" \".join([\"<start>\", id2word[next_word_idx]])]\n",
    "            ## собираем биграмму из <start> и сгенерированного слова\n",
    "            \n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для сравнения - оригинальный вариант для генерации по биграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравним генерацию с триграммами и биграммами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trigram Model\n",
      "==================================================\n",
      "я сую тебе за щеку \n",
      " по потерям хохлов это не револююция а русский язык для старта языки на которых танк ебет на 5 лет подрались даже не знают и умеют больше чем цис-тянки \n",
      " свободный человек и без всяких своих субъективных оценочных суждений \n",
      " the witch cult has been taken down until further notice \n",
      " это уже можно было бы похуй если бы у тебя там пиздецома \n",
      " б-ушные машины стоят копейки бэкдор через фб не работает-проверил вчера \n",
      " химия объясняет как и я ещё тогда удивился физике падения тел уровень он обозревал тот что за тест \n",
      " только с сегой а значит братушки хотят тебя выпилить\n",
      "==================================================\n",
      "Bigram Model\n",
      "==================================================\n",
      "я пруфану бицуху после одного я в мужском роде был \n",
      " libreoffice либеральный анон \n",
      " тебе как они потом когда будешь делать на хуец мой владыка с няса или на расстройство \n",
      " на двачеокажется онанист однако он там русским который не тупой и ракетами-носителями \n",
      " the health news джве школы пионерия мертва рррряяя наносим ответный огонь \n",
      " у меня больше чем то святое \n",
      " как рассказать о комплексах и агрегатами а у всех рэперов может получиться 0 \n",
      " пришлось их непричастности к чему \n",
      " ну что нам свою грязную игру \n",
      " правильно нехуй нахуй вступил в полной мере \n",
      " только\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "print(\"=\"*50)\n",
    "print(\"Trigram Model\")\n",
    "print(\"=\"*50)\n",
    "print(generate_trigrams(matrix_dvach, id2word_dvach, word2id_dvach).replace('<end>', '\\n'))\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"=\"*50)\n",
    "print(\"Bigram Model\")\n",
    "print(\"=\"*50)\n",
    "print(generate_bigrams(matrix_dvach, id2word_dvach, word2id_dvach).replace(\"<end>\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Trigram Model\n",
      "==================================================\n",
      "из оружейного тайника где басаев хранил документы компрометирующие других главарей бандформирований перенести боевые действия \n",
      " по словам иванова речь не идет \n",
      " так не ведет войну в чечне владимир путин выступил перед представителями компаний boeing rocketdyne hughes и rockwell \n",
      " в течение года вкладывает в развитие ряда отраслей экономики \n",
      " sotheby s сталкивается сегодня с вынесением определения судебной коллегии по гражданским делам мосгорсуда решение межмуниципального суда москвы согласно которому юная уильямс должна продолжать выступать в кубке уефа \n",
      " как заявил рбк заместитель начальника российского бюро интерпола алексея абрамова сотрудники мвд не готово заключить честную сделку в интересах следствия \n",
      " в свою очередь отрицают какую-либо причастность к взрывам\n",
      "==================================================\n",
      "Bigram Model\n",
      "==================================================\n",
      "из контрабандной иракской нефти \n",
      " по его политическом пространстве института \n",
      " рнк-компьютер разработанный на своих акций был кошман заявил что покушение с собой гибель большой мощности \n",
      " еще 2 \n",
      " председателя комиссии \n",
      " 8,8 436 являются эксперты по налогам и что именно в воде \n",
      " целью выяснить был сделан по неустановленным пока точно указаны фамилии кучеренко \n",
      " по его представитель госдепартамента сша поддерживают сергей шойгу был задержан член комиссии сформированной горсоветом и с историей спасения аварийных ситуациях \n",
      " как стало голосование проходит до этого россия нет оснований говорить примут участие в банковских переводов миллиардов рублей на американскую компанию \n",
      " в\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "print(\"=\"*50)\n",
    "print(\"Trigram Model\")\n",
    "print(\"=\"*50)\n",
    "print(generate_trigrams(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"=\"*50)\n",
    "print(\"Bigram Model\")\n",
    "print(\"=\"*50)\n",
    "print(generate_bigrams(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "--------------\n",
    "Действительно, триграммная модель генерирует гораздо более связаный текст, чем модель на биграммах. Однако даже триграммная модель ещё далека от того, чтобы генрировать длинные адекватно-связанные тексты. Очевидно это будет улучшаться с добавлением следующих уровней n-граммности, однако тут встает вопрос с ограниченным объемом оперативной памяти.\n",
    "\n",
    "P.S.: я решил не добавлять ещё один тег //<start//> в начало, вместо этого я догенерирую второй токен в начале предложения в униграммном 'режиме'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
