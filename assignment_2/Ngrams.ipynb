{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковое моделирование заключается в приписывании вероятности последовательности слов. Сейчас языковые модели используются практически во всех nlp задачах. Всякие Берты и Элмо - языковые модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем два разных корпуса: новостной и сообщения с 2ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "dvach = open('data/2ch_corpus.txt').read()\n",
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "\n",
    "news = open('data/lenta.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По длине оно сопоставимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 1 - 11638405\n",
      "Длина 2 - 11536552\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина 1 -\", len(dvach))\n",
    "print(\"Длина 2 -\", len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию для нормализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним тексты по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса токсичных постов в токенах - 1858941\n",
      "Длина корпуса новостных текстов в токенах -  1505789\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И по уникальным токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных токенов в корпусе токсичных постов - 189515\n",
      "Уникальный токенов в корпусе новостных текстов -  116302\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных токенов в корпусе токсичных постов -\", len(set(norm_dvach)))\n",
    "print(\"Уникальный токенов в корпусе новостных текстов - \", len(set(norm_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 55892),\n",
       " ('в', 48853),\n",
       " ('не', 46602),\n",
       " ('на', 29660),\n",
       " ('что', 26668),\n",
       " ('я', 21734),\n",
       " ('а', 21310),\n",
       " ('с', 21080),\n",
       " ('это', 17727),\n",
       " ('ты', 15469)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 72412),\n",
       " ('и', 33290),\n",
       " ('на', 28434),\n",
       " ('по', 19490),\n",
       " ('что', 17031),\n",
       " ('с', 15921),\n",
       " ('не', 12702),\n",
       " ('из', 7727),\n",
       " ('о', 7515),\n",
       " ('как', 7514)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнивать употребимость конкретных слов в разных текстах в абсолютных числах неудобно. Нормализуем счётчики на размеры текстов. Так у нас получается вероятность слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.030066580918921042),\n",
       " ('в', 0.02628001641794979),\n",
       " ('не', 0.02506911192985684),\n",
       " ('на', 0.015955320798239428),\n",
       " ('что', 0.014345802260534357),\n",
       " ('я', 0.011691602907246653),\n",
       " ('а', 0.011463516055646737),\n",
       " ('с', 0.011339789697467536),\n",
       " ('это', 0.009536074571489897),\n",
       " ('ты', 0.008321404498582796),\n",
       " ('как', 0.007882444897390503),\n",
       " ('у', 0.006848522895562581),\n",
       " ('но', 0.005786090037284669),\n",
       " ('так', 0.005383172462170666),\n",
       " ('по', 0.005060945990217011),\n",
       " ('то', 0.005049649235774562),\n",
       " ('все', 0.0046537248896011225),\n",
       " ('за', 0.004583792600195488),\n",
       " ('же', 0.004228751746289958),\n",
       " ('если', 0.004209385881531474)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_dvach.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04808907489694771),\n",
       " ('и', 0.0221080111489724),\n",
       " ('на', 0.018883123731146926),\n",
       " ('по', 0.012943380513471676),\n",
       " ('что', 0.011310349590812525),\n",
       " ('с', 0.01057319451795703),\n",
       " ('не', 0.008435444806676101),\n",
       " ('из', 0.005131529052211166),\n",
       " ('о', 0.00499073907433246),\n",
       " ('как', 0.0049900749706632205),\n",
       " ('к', 0.00407161959610543),\n",
       " ('за', 0.0040125143695431435),\n",
       " ('россии', 0.0036751497055696383),\n",
       " ('для', 0.003325831175549828),\n",
       " ('его', 0.003260084912295149),\n",
       " ('он', 0.0031704309169478593),\n",
       " ('от', 0.003066830744546547),\n",
       " ('сообщает', 0.003050228152815567),\n",
       " ('а', 0.0029180715226369697),\n",
       " ('также', 0.002716184007188258)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти вероятности уже можно использовать, чтобы ответить на вопрос - это предложение больше подходит для новостей или для анонимного форума?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
    "\n",
    "prob = Counter({'news':0, 'dvach':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['dvach'] += probas_dvach.get(word, 0)\n",
    "    prob['news'] += probas_news.get(word, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dvach', 0.034328147047162874), ('news', 0.014186582582287426)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'как вы смотрите эту залупу, серьезно. в чем прикол ваще это смотреть?'\n",
    "\n",
    "prob = Counter({'news':0, 'dvach':0})\n",
    "\n",
    "for word in normalize(phrase):\n",
    "    prob['dvach'] += probas_dvach.get(word, 0)\n",
    "    prob['news'] += probas_news.get(word, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('news', 0.05619313197267346), ('dvach', 0.04750823183737408)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты получаются не очень точные. Возможно это из-за того, что мы считаем слова независимыми друг от друга. А это очевидно не так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-хорошему вероятность последовательности нужно расчитывать по формуле полной вероятности. Но у нас не очень большие тексты и мы не можем получить вероятности для длинных фраз (их просто может не быть в текстах). Поэтому мы воспользуемся предположением Маркова и будем учитывать только предыдущее слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
    "\n",
    "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
    "\n",
    "Ну и поделим все на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f27552cfe849609f8f0c678a2de249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=170507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences_dvach = [['<start>'] + normalize(text) + ['<end>'] for text in tqdm(sent_tokenize(dvach))]\n",
    "sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in tqdm(sent_tokenize(news))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189517"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigrams_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> в', 7972),\n",
       " ('<start> по', 6211),\n",
       " ('<start> как', 3738),\n",
       " ('риа новости', 3504),\n",
       " ('по словам', 1971),\n",
       " ('об этом', 1795),\n",
       " ('<start> однако', 1694),\n",
       " ('<start> на', 1643),\n",
       " ('что в', 1624),\n",
       " ('<start> об', 1619)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала''Ныть надо меньше и работать больше.'\n",
    "# phrase = 'как вы смотрите эту залупу, серьезно. в чем прикол ваще это смотреть?'\n",
    "prob = Counter()\n",
    "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
    "    word1, word2 = ngram.split()\n",
    "    \n",
    "    if word1 in unigrams_dvach and ngram in bigrams_dvach:\n",
    "        prob['dvach'] += np.log(bigrams_dvach[ngram]/unigrams_dvach[word1])\n",
    "    else:\n",
    "        prob['dvach'] += np.log(0.001)\n",
    "    \n",
    "    if word1 in unigrams_news and ngram in bigrams_news:\n",
    "        prob['news'] += np.log(bigrams_news[ngram]/unigrams_news[word1])\n",
    "    else:\n",
    "        prob['news'] += np.log(0.001)\n",
    "\n",
    "prob['news'] = np.exp(prob['news'])\n",
    "prob['dvach'] = np.exp(prob['dvach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('news', 3.3368639658312657e-41), ('dvach', 4.888715771763858e-53)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает получше. Мы воспользовались небольшим хаком - для слов или биграммов, которых не было у нас в словаре, прибавляли низкую вероятность. Исправить это по-нормальному - сложно, придется подробнее разбираться с вероятностями, сглаживаниями и заменой неизвестных слов. Если интрересно - в книге Журафского про это есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблем с неизвестными словами у нас не будет, если мы будем пытаться сгенерировать новый текст. Давайте попробуем это сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = np.zeros((len(unigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "\n",
    "for ngram in bigrams_dvach:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_dvach[word2id_dvach[word1]][word2id_dvach[word2]] =  (bigrams_dvach[ngram]/\n",
    "                                                                     unigrams_dvach[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим матрицу вероятностей перейти из 1 слов в другое\n",
    "matrix_news = np.zeros((len(unigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in bigrams_news:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_news[word2id_news[word1]][word2id_news[word2]] =  (bigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "они потеряли со всеми \n",
      " молодец \n",
      " а остальные 4 имя той толпы следовательно вставлять как только стареть и фаерфаксе свг спрайт \n",
      " и пропустить что это чтоб задыхалась так будь ты хуй на 10 назад \n",
      " от 20 черепов – я посмотрю как худая смотрит \n",
      " с четвёртого энергоблока \n",
      " одним выстрелом расхуярить чтобы было видно что совершил преступление перед прыжком скажу только малая зона в дни месяцы слабо подумать что была мечта казуала \n",
      " с крайностями \n",
      " блин сделали потому что он спит а не дна хуже чем китай \n",
      " год поставил \n",
      " св \n",
      " геноцида ксеносов в\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, word2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "иначе представители таких как сми сказал генерал колонна из юар нельсон автор сюжета было совершено покушение на своих наблюдателей передает риа новости \n",
      " по полному расчету с первым шагом на 15 сентября \n",
      " на средства буквально за рубежом \n",
      " полностью поддержала точку зрения на внешних врагов то будут очень похож на политический советник президента около ста семей \n",
      " владелец предприятия \n",
      " касаясь ситуации в длинном перечне 85 лет и оружия был обнаружен тайник с 1993 по умолчанию не поможет ведь никто не имеет самые решительные действия в пресс-службе всемирного банка \n",
      " в порту сочи началась эпидемия гриппа достигнет рекордного уровня\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать триграммную модель на основе кода выше.\n",
    "\n",
    "Подсказки:\n",
    "    - нужно будет добавить еще один тэг <start>\n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы\n",
    "    - тексты должны быть очень похоже на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
